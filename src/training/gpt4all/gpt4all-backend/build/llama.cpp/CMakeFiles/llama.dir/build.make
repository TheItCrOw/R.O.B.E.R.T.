# CMAKE generated file: DO NOT EDIT!
# Generated by "MinGW Makefiles" Generator, CMake Version 3.26

# Delete rule output on recipe failure.
.DELETE_ON_ERROR:

#=============================================================================
# Special targets provided by cmake.

# Disable implicit rules so canonical targets will work.
.SUFFIXES:

# Disable VCS-based implicit rules.
% : %,v

# Disable VCS-based implicit rules.
% : RCS/%

# Disable VCS-based implicit rules.
% : RCS/%,v

# Disable VCS-based implicit rules.
% : SCCS/s.%

# Disable VCS-based implicit rules.
% : s.%

.SUFFIXES: .hpux_make_needs_suffix_list

# Produce verbose output by default.
VERBOSE = 1

# Command-line flag to silence nested $(MAKE).
$(VERBOSE)MAKESILENT = -s

#Suppress display of executed commands.
$(VERBOSE).SILENT:

# A target that is always out of date.
cmake_force:
.PHONY : cmake_force

#=============================================================================
# Set environment variables for the build.

SHELL = cmd.exe

# The CMake executable.
CMAKE_COMMAND = C:\tools\mingw64\bin\cmake.exe

# The command to remove a file.
RM = C:\tools\mingw64\bin\cmake.exe -E rm -f

# Escaping for special characters.
EQUALS = =

# The top-level source directory on which CMake was run.
CMAKE_SOURCE_DIR = "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend"

# The top-level build directory on which CMake was run.
CMAKE_BINARY_DIR = "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build"

# Include any dependencies generated for this target.
include llama.cpp/CMakeFiles/llama.dir/depend.make
# Include any dependencies generated by the compiler for this target.
include llama.cpp/CMakeFiles/llama.dir/compiler_depend.make

# Include the progress variables for this target.
include llama.cpp/CMakeFiles/llama.dir/progress.make

# Include the compile flags for this target's objects.
include llama.cpp/CMakeFiles/llama.dir/flags.make

llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj: llama.cpp/CMakeFiles/llama.dir/flags.make
llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj: llama.cpp/CMakeFiles/llama.dir/includes_CXX.rsp
llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj: E:/Python\ Projects/R.O.B.E.R.T/source/R.O.B.E.R.T/src/training/gpt4all/gpt4all-backend/llama.cpp/llama.cpp
llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj: llama.cpp/CMakeFiles/llama.dir/compiler_depend.ts
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green --progress-dir="E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\CMakeFiles" --progress-num=$(CMAKE_PROGRESS_1) "Building CXX object llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\c++.exe $(CXX_DEFINES) $(CXX_INCLUDES) $(CXX_FLAGS) -MD -MT llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj -MF CMakeFiles\llama.dir\llama.cpp.obj.d -o CMakeFiles\llama.dir\llama.cpp.obj -c "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\llama.cpp"

llama.cpp/CMakeFiles/llama.dir/llama.cpp.i: cmake_force
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green "Preprocessing CXX source to CMakeFiles/llama.dir/llama.cpp.i"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\c++.exe $(CXX_DEFINES) $(CXX_INCLUDES) $(CXX_FLAGS) -E "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\llama.cpp" > CMakeFiles\llama.dir\llama.cpp.i

llama.cpp/CMakeFiles/llama.dir/llama.cpp.s: cmake_force
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green "Compiling CXX source to assembly CMakeFiles/llama.dir/llama.cpp.s"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\c++.exe $(CXX_DEFINES) $(CXX_INCLUDES) $(CXX_FLAGS) -S "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\llama.cpp" -o CMakeFiles\llama.dir\llama.cpp.s

# Object files for target llama
llama_OBJECTS = \
"CMakeFiles/llama.dir/llama.cpp.obj"

# External object files for target llama
llama_EXTERNAL_OBJECTS = \
"E:/Python Projects/R.O.B.E.R.T/source/R.O.B.E.R.T/src/training/gpt4all/gpt4all-backend/build/llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj"

bin/libllama.dll: llama.cpp/CMakeFiles/llama.dir/llama.cpp.obj
bin/libllama.dll: llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj
bin/libllama.dll: llama.cpp/CMakeFiles/llama.dir/build.make
bin/libllama.dll: llama.cpp/CMakeFiles/llama.dir/linkLibs.rsp
bin/libllama.dll: llama.cpp/CMakeFiles/llama.dir/objects1.rsp
bin/libllama.dll: llama.cpp/CMakeFiles/llama.dir/link.txt
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green --bold --progress-dir="E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\CMakeFiles" --progress-num=$(CMAKE_PROGRESS_2) "Linking CXX shared library ..\bin\libllama.dll"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && $(CMAKE_COMMAND) -E cmake_link_script CMakeFiles\llama.dir\link.txt --verbose=$(VERBOSE)

# Rule to build all files generated by this target.
llama.cpp/CMakeFiles/llama.dir/build: bin/libllama.dll
.PHONY : llama.cpp/CMakeFiles/llama.dir/build

llama.cpp/CMakeFiles/llama.dir/clean:
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && $(CMAKE_COMMAND) -P CMakeFiles\llama.dir\cmake_clean.cmake
.PHONY : llama.cpp/CMakeFiles/llama.dir/clean

llama.cpp/CMakeFiles/llama.dir/depend:
	$(CMAKE_COMMAND) -E cmake_depends "MinGW Makefiles" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp\CMakeFiles\llama.dir\DependInfo.cmake" --color=$(COLOR)
.PHONY : llama.cpp/CMakeFiles/llama.dir/depend

