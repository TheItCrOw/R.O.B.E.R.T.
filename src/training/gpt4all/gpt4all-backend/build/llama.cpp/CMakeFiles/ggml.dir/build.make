# CMAKE generated file: DO NOT EDIT!
# Generated by "MinGW Makefiles" Generator, CMake Version 3.26

# Delete rule output on recipe failure.
.DELETE_ON_ERROR:

#=============================================================================
# Special targets provided by cmake.

# Disable implicit rules so canonical targets will work.
.SUFFIXES:

# Disable VCS-based implicit rules.
% : %,v

# Disable VCS-based implicit rules.
% : RCS/%

# Disable VCS-based implicit rules.
% : RCS/%,v

# Disable VCS-based implicit rules.
% : SCCS/s.%

# Disable VCS-based implicit rules.
% : s.%

.SUFFIXES: .hpux_make_needs_suffix_list

# Produce verbose output by default.
VERBOSE = 1

# Command-line flag to silence nested $(MAKE).
$(VERBOSE)MAKESILENT = -s

#Suppress display of executed commands.
$(VERBOSE).SILENT:

# A target that is always out of date.
cmake_force:
.PHONY : cmake_force

#=============================================================================
# Set environment variables for the build.

SHELL = cmd.exe

# The CMake executable.
CMAKE_COMMAND = C:\tools\mingw64\bin\cmake.exe

# The command to remove a file.
RM = C:\tools\mingw64\bin\cmake.exe -E rm -f

# Escaping for special characters.
EQUALS = =

# The top-level source directory on which CMake was run.
CMAKE_SOURCE_DIR = "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend"

# The top-level build directory on which CMake was run.
CMAKE_BINARY_DIR = "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build"

# Include any dependencies generated for this target.
include llama.cpp/CMakeFiles/ggml.dir/depend.make
# Include any dependencies generated by the compiler for this target.
include llama.cpp/CMakeFiles/ggml.dir/compiler_depend.make

# Include the progress variables for this target.
include llama.cpp/CMakeFiles/ggml.dir/progress.make

# Include the compile flags for this target's objects.
include llama.cpp/CMakeFiles/ggml.dir/flags.make

llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj: llama.cpp/CMakeFiles/ggml.dir/flags.make
llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj: llama.cpp/CMakeFiles/ggml.dir/includes_C.rsp
llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj: E:/Python\ Projects/R.O.B.E.R.T/source/R.O.B.E.R.T/src/training/gpt4all/gpt4all-backend/llama.cpp/ggml.c
llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj: llama.cpp/CMakeFiles/ggml.dir/compiler_depend.ts
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green --progress-dir="E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\CMakeFiles" --progress-num=$(CMAKE_PROGRESS_1) "Building C object llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\gcc.exe $(C_DEFINES) $(C_INCLUDES) $(C_FLAGS) -MD -MT llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj -MF CMakeFiles\ggml.dir\ggml.c.obj.d -o CMakeFiles\ggml.dir\ggml.c.obj -c "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\ggml.c"

llama.cpp/CMakeFiles/ggml.dir/ggml.c.i: cmake_force
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green "Preprocessing C source to CMakeFiles/ggml.dir/ggml.c.i"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\gcc.exe $(C_DEFINES) $(C_INCLUDES) $(C_FLAGS) -E "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\ggml.c" > CMakeFiles\ggml.dir\ggml.c.i

llama.cpp/CMakeFiles/ggml.dir/ggml.c.s: cmake_force
	@$(CMAKE_COMMAND) -E cmake_echo_color --switch=$(COLOR) --green "Compiling C source to assembly CMakeFiles/ggml.dir/ggml.c.s"
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && C:\tools\mingw64\bin\gcc.exe $(C_DEFINES) $(C_INCLUDES) $(C_FLAGS) -S "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp\ggml.c" -o CMakeFiles\ggml.dir\ggml.c.s

ggml: llama.cpp/CMakeFiles/ggml.dir/ggml.c.obj
ggml: llama.cpp/CMakeFiles/ggml.dir/build.make
.PHONY : ggml

# Rule to build all files generated by this target.
llama.cpp/CMakeFiles/ggml.dir/build: ggml
.PHONY : llama.cpp/CMakeFiles/ggml.dir/build

llama.cpp/CMakeFiles/ggml.dir/clean:
	cd /d "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" && $(CMAKE_COMMAND) -P CMakeFiles\ggml.dir\cmake_clean.cmake
.PHONY : llama.cpp/CMakeFiles/ggml.dir/clean

llama.cpp/CMakeFiles/ggml.dir/depend:
	$(CMAKE_COMMAND) -E cmake_depends "MinGW Makefiles" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\llama.cpp" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp" "E:\Python Projects\R.O.B.E.R.T\source\R.O.B.E.R.T\src\training\gpt4all\gpt4all-backend\build\llama.cpp\CMakeFiles\ggml.dir\DependInfo.cmake" --color=$(COLOR)
.PHONY : llama.cpp/CMakeFiles/ggml.dir/depend

